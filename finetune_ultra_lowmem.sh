#!/bin/bash

# æ¿€æ´» conda ç¯å¢ƒ
source ~/miniconda3/etc/profile.d/conda.sh
conda activate rdt

# ==========================================
# RDT-1B å¾®è°ƒè„šæœ¬ - æé™çœæ˜¾å­˜ç‰ˆæœ¬
# ä½¿ç”¨ DeepSpeed ZeRO-3 + CPU Offload
# é€‚ç”¨äº: RTX 4090 16GB
# ==========================================

export TEXT_ENCODER_NAME="google/t5-v1_1-xxl"
export VISION_ENCODER_NAME="google/siglip-so400m-patch14-384"
export OUTPUT_DIR="./checkpoints/rdt-1b-stack-blocks"
export WANDB_PROJECT="rdt-stack-blocks-finetune"

# PyTorch å†…å­˜ä¼˜åŒ–
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:128"

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p "$OUTPUT_DIR"

echo "================================================"
echo "ğŸš€ RDT-1B å¾®è°ƒ - æé™çœæ˜¾å­˜ç‰ˆæœ¬"
echo "================================================"
echo "âš¡ ä½¿ç”¨ DeepSpeed ZeRO-3 + CPU Offload"
echo "   ä¼˜åŒ–å™¨çŠ¶æ€ -> CPU"
echo "   æ¨¡å‹å‚æ•° -> CPU"
echo "   æ¢¯åº¦ -> åˆ†ç‰‡"
echo "================================================"

accelerate launch main.py \
    --deepspeed="./configs/zero2_offload.json" \
    --pretrained_model_name_or_path="robotics-diffusion-transformer/rdt-1b" \
    --pretrained_text_encoder_name_or_path=$TEXT_ENCODER_NAME \
    --pretrained_vision_encoder_name_or_path=$VISION_ENCODER_NAME \
    --output_dir=$OUTPUT_DIR \
    --train_batch_size=1 \
    --gradient_accumulation_steps=16 \
    --sample_batch_size=1 \
    --max_train_steps=50000 \
    --checkpointing_period=2000 \
    --sample_period=2000 \
    --checkpoints_total_limit=5 \
    --lr_scheduler="constant" \
    --learning_rate=5e-5 \
    --mixed_precision="bf16" \
    --dataloader_num_workers=2 \
    --image_aug \
    --dataset_type="finetune" \
    --state_noise_snr=40 \
    --load_from_hdf5 \
    --precomp_lang_embed \
    --report_to=wandb

# å…³é”®ä¼˜åŒ–ï¼š
# 1. ZeRO-2 + CPU Offload: ä¼˜åŒ–å™¨å’Œå‚æ•°å¸è½½åˆ°CPU
# 2. Batch Size = 1: æœ€å°batch size
# 3. Gradient Accumulation = 16: ç´¯ç§¯æ›´å¤šæ­¥éª¤
# 4. Gradient Checkpointing: é‡è®¡ç®—ä»£æ›¿å­˜å‚¨
# 5. å‡å°‘éªŒè¯é¢‘ç‡: sample_period=2000
# 6. å‡å°‘checkpointæ•°é‡: checkpoints_total_limit=5

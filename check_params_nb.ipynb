{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5f0fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.rdt.model import RDT\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "try:\n",
    "    # Guess 1: Maybe 170M is much smaller?\n",
    "    model_guess_1 = RDT(\n",
    "        hidden_size=768,\n",
    "        depth=12,\n",
    "        num_heads=12,\n",
    "    )\n",
    "    \n",
    "    # Guess 2: \n",
    "    model_guess_2 = RDT(\n",
    "        hidden_size=1024,\n",
    "        depth=12,\n",
    "        num_heads=16,\n",
    "    )\n",
    "\n",
    "    # The default in file\n",
    "    model_file_default = RDT(hidden_size=1152, depth=28, num_heads=16)\n",
    "\n",
    "    # 1B config from base.yaml\n",
    "    model_1b = RDT(\n",
    "        hidden_size=2048,\n",
    "        depth=28,\n",
    "        num_heads=32,\n",
    "    )\n",
    "\n",
    "    print(f\"Guess 1 (768/12): {count_parameters(model_guess_1) / 1e6:.2f}M\")\n",
    "    print(f\"Guess 2 (1024/12): {count_parameters(model_guess_2) / 1e6:.2f}M\")\n",
    "    print(f\"File default (1152/28): {count_parameters(model_file_default) / 1e6:.2f}M\")\n",
    "    print(f\"RDT-1B model params: {count_parameters(model_1b) / 1e6:.2f}M\")\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
